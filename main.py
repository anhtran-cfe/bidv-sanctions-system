#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Main workflow for Sanctions Processing System
Processes PDF files through a complete pipeline to extract sanctions data
Updated workflow: PDF ‚Üí MD ‚Üí Base64 ‚Üí CSV (via Gemini API)
Supports: Single PDF or Multiple PDFs batch processing
"""

import os
import sys
import glob
import pandas as pd
from datetime import datetime
import re
import subprocess
import time
import base64
from typing import List, Dict, Any
from pathlib import Path

# Import c√°c module kh√°c
try:
    import pdf_to_md
    from markdown_to_base64 import MarkdownBase64Converter
    from gemini_markdown_csv import GeminiMarkdownToCSVConverter
    import ofac_extractor
    import un_sanctions_parser
except ImportError as e:
    print(f"‚ùå L·ªói import module: {e}")
    print("Vui l√≤ng ƒë·∫£m b·∫£o t·∫•t c·∫£ c√°c file Python c·∫ßn thi·∫øt c√≥ trong c√πng th∆∞ m·ª•c")
    print("C·∫ßn c√≥: pdf_to_md.py, markdown_to_base64.py, gemini_markdown_csv.py")
    sys.exit(1)

class SanctionsWorkflow:
    def __init__(self):
        self.pdf_files = []  # Changed to support multiple files
        self.processing_mode = 'single'  # 'single' or 'batch'
        self.timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        self.final_output = f"sanctions_cleaned_{self.timestamp}.csv"
        self.temp_files = []
        self.gemini_api_key = None
        self.processed_pdfs = []  # Track processed PDFs info
        
    def print_header(self, title):
        """In header cho t·ª´ng b∆∞·ªõc"""
        print("\n" + "="*80)
        print(f"üîÑ {title}")
        print("="*80)
        
    def print_step(self, step_num, title):
        """In th√¥ng tin b∆∞·ªõc hi·ªán t·∫°i"""
        print(f"\nüìã B∆Ø·ªöC {step_num}: {title}")
        print("-" * 60)
        
    def get_processing_mode(self):
        """Ch·ªçn ch·∫ø ƒë·ªô x·ª≠ l√Ω: single ho·∫∑c batch"""
        print("üìã CH·ªåN CH·∫æA ƒê·ªò X·ª¨ L√ù")
        print("-" * 30)
        print("1. X·ª≠ l√Ω 1 file PDF")
        print("2. X·ª≠ l√Ω nhi·ªÅu file PDF (batch)")
        print("3. X·ª≠ l√Ω t·∫•t c·∫£ PDF trong th∆∞ m·ª•c")
        
        while True:
            try:
                choice = input("\nCh·ªçn ch·∫ø ƒë·ªô (1-3): ").strip()
                
                if choice == '1':
                    self.processing_mode = 'single'
                    return self.get_single_pdf_input()
                elif choice == '2':
                    self.processing_mode = 'batch'
                    return self.get_multiple_pdf_input()
                elif choice == '3':
                    self.processing_mode = 'batch'
                    return self.get_all_pdf_in_directory()
                else:
                    print("‚ùå Vui l√≤ng ch·ªçn 1, 2 ho·∫∑c 3")
                    
            except KeyboardInterrupt:
                print("\n‚ùå ƒê√£ h·ªßy")
                return False
    
    def get_single_pdf_input(self):
        """L·∫•y 1 file PDF t·ª´ ng∆∞·ªùi d√πng"""
        print("\nüìÅ NH·∫¨P TH√îNG TIN FILE PDF")
        print("-" * 30)
        
        while True:
            pdf_name = input("Nh·∫≠p t√™n file PDF (v√≠ d·ª•: 202501578.pdf): ").strip()
            
            if not pdf_name:
                print("‚ùå Vui l√≤ng nh·∫≠p t√™n file PDF")
                continue
                
            if not pdf_name.endswith('.pdf'):
                pdf_name += '.pdf'
                
            if not os.path.exists(pdf_name):
                print(f"‚ùå File {pdf_name} kh√¥ng t·ªìn t·∫°i")
                self.show_available_pdfs()
                
                try:
                    choice = input("\nCh·ªçn s·ªë th·ª© t·ª± file ho·∫∑c nh·∫≠p t√™n kh√°c (Enter ƒë·ªÉ nh·∫≠p l·∫°i): ").strip()
                    if choice.isdigit():
                        pdf_files = glob.glob("*.pdf")
                        if 1 <= int(choice) <= len(pdf_files):
                            pdf_name = pdf_files[int(choice) - 1]
                            break
                except:
                    pass
                continue
            else:
                break
                
        self.pdf_files = [pdf_name]
        print(f"‚úÖ ƒê√£ ch·ªçn file: {pdf_name}")
        return True
    
    def get_multiple_pdf_input(self):
        """L·∫•y nhi·ªÅu file PDF t·ª´ ng∆∞·ªùi d√πng"""
        print("\nüìÅ NH·∫¨P DANH S√ÅCH FILE PDF")
        print("-" * 30)
        print("üí° C√°c c√°ch nh·∫≠p:")
        print("   - Nh·∫≠p t·ª´ng t√™n file, c√°ch nhau b·ªüi d·∫•u ph·∫©y")
        print("   - Nh·∫≠p s·ªë th·ª© t·ª± t·ª´ danh s√°ch d∆∞·ªõi ƒë√¢y")
        print("   - Ho·∫∑c nh·∫≠p 'all' ƒë·ªÉ ch·ªçn t·∫•t c·∫£")
        
        self.show_available_pdfs()
        
        while True:
            user_input = input("\nNh·∫≠p danh s√°ch PDF: ").strip()
            
            if not user_input:
                print("‚ùå Vui l√≤ng nh·∫≠p danh s√°ch file PDF")
                continue
            
            pdf_files_in_dir = glob.glob("*.pdf")
            selected_files = []
            
            if user_input.lower() == 'all':
                selected_files = pdf_files_in_dir
                
            elif ',' in user_input:
                # Nh·∫≠p danh s√°ch t√™n file
                file_names = [name.strip() for name in user_input.split(',')]
                for name in file_names:
                    if not name.endswith('.pdf'):
                        name += '.pdf'
                    if os.path.exists(name):
                        selected_files.append(name)
                    else:
                        print(f"‚ö†Ô∏è File kh√¥ng t·ªìn t·∫°i: {name}")
                        
            else:
                # Nh·∫≠p s·ªë th·ª© t·ª± (c√≥ th·ªÉ nhi·ªÅu s·ªë c√°ch nhau b·ªüi space ho·∫∑c comma)
                try:
                    if ',' in user_input:
                        numbers = [int(n.strip()) for n in user_input.split(',')]
                    else:
                        numbers = [int(n.strip()) for n in user_input.split()]
                    
                    for num in numbers:
                        if 1 <= num <= len(pdf_files_in_dir):
                            selected_files.append(pdf_files_in_dir[num - 1])
                        else:
                            print(f"‚ö†Ô∏è S·ªë th·ª© t·ª± kh√¥ng h·ª£p l·ªá: {num}")
                            
                except ValueError:
                    print("‚ùå Format kh√¥ng ƒë√∫ng. Vui l√≤ng th·ª≠ l·∫°i")
                    continue
            
            if selected_files:
                self.pdf_files = list(set(selected_files))  # Remove duplicates
                print(f"\n‚úÖ ƒê√£ ch·ªçn {len(self.pdf_files)} file PDF:")
                for i, file in enumerate(self.pdf_files, 1):
                    file_size = os.path.getsize(file)
                    print(f"   {i}. {file} ({file_size:,} bytes)")
                return True
            else:
                print("‚ùå Kh√¥ng c√≥ file n√†o ƒë∆∞·ª£c ch·ªçn. Vui l√≤ng th·ª≠ l·∫°i")
    
    def get_all_pdf_in_directory(self):
        """L·∫•y t·∫•t c·∫£ file PDF trong th∆∞ m·ª•c"""
        pdf_files = glob.glob("*.pdf")
        
        if not pdf_files:
            print("‚ùå Kh√¥ng t√¨m th·∫•y file PDF n√†o trong th∆∞ m·ª•c")
            return False
            
        self.pdf_files = pdf_files
        print(f"\n‚úÖ T√¨m th·∫•y {len(pdf_files)} file PDF:")
        
        total_size = 0
        for i, file in enumerate(pdf_files, 1):
            file_size = os.path.getsize(file)
            total_size += file_size
            print(f"   {i}. {file} ({file_size:,} bytes)")
            
        print(f"\nüìä T·ªïng k√≠ch th∆∞·ªõc: {total_size:,} bytes ({total_size/1024/1024:.1f} MB)")
        
        # X√°c nh·∫≠n x·ª≠ l√Ω t·∫•t c·∫£
        try:
            confirm = input(f"\nü§î X·ª≠ l√Ω t·∫•t c·∫£ {len(pdf_files)} file PDF? (Y/n): ").strip().lower()
            if confirm and confirm not in ['y', 'yes']:
                return False
        except KeyboardInterrupt:
            return False
            
        return True
    
    def show_available_pdfs(self):
        """Hi·ªÉn th·ªã danh s√°ch PDF c√≥ s·∫µn"""
        pdf_files = glob.glob("*.pdf")
        if pdf_files:
            print(f"\nüìã {len(pdf_files)} file PDF c√≥ s·∫µn:")
            for i, file in enumerate(pdf_files, 1):
                file_size = os.path.getsize(file)
                mod_time = datetime.fromtimestamp(os.path.getmtime(file))
                print(f"   {i}. {file} ({file_size:,} bytes, {mod_time.strftime('%Y-%m-%d %H:%M')})")
        else:
            print("   ‚ùå Kh√¥ng t√¨m th·∫•y file PDF n√†o")
    
    def get_gemini_api_key(self):
        """L·∫•y Gemini API key"""
        # Ki·ªÉm tra environment variable tr∆∞·ªõc
        api_key = os.getenv('GEMINI_API_KEY')
        
        if api_key:
            self.gemini_api_key = api_key
            print("‚úÖ ƒê√£ t√¨m th·∫•y Gemini API key t·ª´ environment variable")
            return True
        
        # N·∫øu kh√¥ng c√≥, y√™u c·∫ßu nh·∫≠p
        print("\nüîë THI·∫æT L·∫¨P GEMINI API KEY")
        print("-" * 40)
        print("üí° B·∫°n c·∫ßn Gemini API key ƒë·ªÉ x·ª≠ l√Ω d·ªØ li·ªáu PDF")
        print("   L·∫•y API key t·∫°i: https://aistudio.google.com/app/apikey")
        
        if self.processing_mode == 'batch':
            print(f"‚ö†Ô∏è ƒêang x·ª≠ l√Ω {len(self.pdf_files)} file PDF - c·∫ßn API key ƒë·ªÉ x·ª≠ l√Ω hi·ªáu qu·∫£")
        
        while True:
            api_key = input("\nNh·∫≠p Gemini API key (ho·∫∑c 'skip' ƒë·ªÉ b·ªè qua): ").strip()
            
            if api_key.lower() == 'skip':
                print("‚ö†Ô∏è B·ªè qua Gemini API - ch·ªâ x·ª≠ l√Ω OFAC v√† UN data")
                return False
            
            if not api_key:
                print("‚ùå Vui l√≤ng nh·∫≠p API key h·ª£p l·ªá")
                continue
            
            # Ki·ªÉm tra format c∆° b·∫£n c·ªßa API key
            if len(api_key) < 20:
                print("‚ùå API key qu√° ng·∫Øn, vui l√≤ng ki·ªÉm tra l·∫°i")
                continue
            
            self.gemini_api_key = api_key
            print("‚úÖ ƒê√£ thi·∫øt l·∫≠p Gemini API key")
            return True
    
    def step1_pdf_to_md_batch(self):
        """B∆∞·ªõc 1: Chuy·ªÉn t·∫•t c·∫£ PDF sang Markdown"""
        self.print_step(1, f"CHUY·ªÇN ƒê·ªîI {len(self.pdf_files)} PDF SANG MARKDOWN")
        
        successful_conversions = 0
        conversion_results = []
        
        for i, pdf_file in enumerate(self.pdf_files, 1):
            print(f"\nüìÑ [{i}/{len(self.pdf_files)}] ƒêang x·ª≠ l√Ω: {pdf_file}")
            
            try:
                # Import v√† s·ª≠ d·ª•ng pymupdf4llm
                import pymupdf4llm
                
                print(f"   üîÑ ƒêang chuy·ªÉn ƒë·ªïi...")
                md_text = pymupdf4llm.to_markdown(pdf_file)
                
                # T·∫°o t√™n file markdown
                md_filename = f"{os.path.splitext(pdf_file)[0]}.md"
                
                with open(md_filename, "w", encoding="utf-8") as f:
                    f.write(md_text)
                
                self.temp_files.append(md_filename)
                
                # Th√¥ng tin chi ti·∫øt
                file_size = os.path.getsize(md_filename)
                line_count = len(md_text.splitlines())
                
                conversion_result = {
                    'pdf_file': pdf_file,
                    'md_file': md_filename,
                    'success': True,
                    'file_size': file_size,
                    'line_count': line_count,
                    'content_preview': md_text[:200] + "..." if len(md_text) > 200 else md_text
                }
                
                conversion_results.append(conversion_result)
                successful_conversions += 1
                
                print(f"   ‚úÖ Th√†nh c√¥ng: {md_filename}")
                print(f"   üìä K√≠ch th∆∞·ªõc: {file_size:,} bytes, {line_count:,} d√≤ng")
                
            except Exception as e:
                print(f"   ‚ùå L·ªói: {e}")
                conversion_results.append({
                    'pdf_file': pdf_file,
                    'md_file': None,
                    'success': False,
                    'error': str(e)
                })
        
        # T·ªïng k·∫øt b∆∞·ªõc 1
        print(f"\nüìä K·∫æT QU·∫¢ CHUY·ªÇN ƒê·ªîI PDF ‚Üí MD:")
        print(f"   ‚úÖ Th√†nh c√¥ng: {successful_conversions}/{len(self.pdf_files)}")
        print(f"   ‚ùå Th·∫•t b·∫°i: {len(self.pdf_files) - successful_conversions}")
        
        if successful_conversions == 0:
            print("‚ùå Kh√¥ng c√≥ file n√†o ƒë∆∞·ª£c chuy·ªÉn ƒë·ªïi th√†nh c√¥ng")
            return False
        
        # L∆∞u k·∫øt qu·∫£ ƒë·ªÉ s·ª≠ d·ª•ng ·ªü b∆∞·ªõc ti·∫øp theo
        self.conversion_results = conversion_results
        return True
    
    def step2_md_to_base64_batch(self):
        """B∆∞·ªõc 2: Chuy·ªÉn t·∫•t c·∫£ Markdown sang Base64"""
        self.print_step(2, f"CHUY·ªÇN ƒê·ªîI MARKDOWN SANG BASE64")
        
        if not hasattr(self, 'conversion_results'):
            print("‚ùå Kh√¥ng c√≥ d·ªØ li·ªáu markdown t·ª´ b∆∞·ªõc tr∆∞·ªõc")
            return False
        
        successful_conversions = 0
        base64_results = []
        
        converter = MarkdownBase64Converter()
        
        for result in self.conversion_results:
            if not result['success']:
                continue
                
            md_file = result['md_file']
            print(f"\nüìÑ ƒêang x·ª≠ l√Ω: {md_file}")
            
            try:
                print(f"   üîÑ ƒêang chuy·ªÉn ƒë·ªïi sang Base64...")
                
                base64_result = converter.file_to_base64(md_file)
                
                if not base64_result['success']:
                    print(f"   ‚ùå L·ªói: {base64_result['error']}")
                    continue
                
                # Th√™m th√¥ng tin Base64 v√†o result
                result['base64_content'] = base64_result['base64_content']
                result['base64_metadata'] = {
                    'base64_size': base64_result['base64_size'],
                    'encoding': base64_result['encoding']
                }
                
                base64_results.append(result)
                successful_conversions += 1
                
                print(f"   ‚úÖ Th√†nh c√¥ng")
                print(f"   üìä Base64 size: {base64_result['base64_size']:,} k√Ω t·ª±")
                
            except Exception as e:
                print(f"   ‚ùå L·ªói: {e}")
        
        # T·ªïng k·∫øt b∆∞·ªõc 2
        print(f"\nüìä K·∫æT QU·∫¢ CHUY·ªÇN ƒê·ªîI MD ‚Üí BASE64:")
        print(f"   ‚úÖ Th√†nh c√¥ng: {successful_conversions}")
        
        if successful_conversions == 0:
            print("‚ùå Kh√¥ng c√≥ file n√†o ƒë∆∞·ª£c chuy·ªÉn ƒë·ªïi th√†nh c√¥ng")
            return False
        
        self.base64_results = base64_results
        return True
    
    def step3_base64_to_csv_via_gemini_batch(self):
        """B∆∞·ªõc 3: Chuy·ªÉn t·∫•t c·∫£ Base64 sang CSV qua Gemini API"""
        self.print_step(3, f"X·ª¨ L√ù D·ªÆ LI·ªÜU V·ªöI GEMINI API (BASE64 ‚Üí CSV)")
        
        if not hasattr(self, 'base64_results'):
            print("‚ùå Kh√¥ng c√≥ d·ªØ li·ªáu Base64 t·ª´ b∆∞·ªõc tr∆∞·ªõc")
            return False
        
        if not self.gemini_api_key:
            print("‚ùå Kh√¥ng c√≥ Gemini API key")
            return False
        
        successful_conversions = 0
        converter = GeminiMarkdownToCSVConverter(self.gemini_api_key)
        
        print(f"ü§ñ ƒêang x·ª≠ l√Ω {len(self.base64_results)} file v·ªõi Gemini AI...")
        print("‚è±Ô∏è L∆∞u √Ω: M·ªói file c√≥ th·ªÉ m·∫•t v√†i ph√∫t ƒë·ªÉ x·ª≠ l√Ω")
        
        for i, result in enumerate(self.base64_results, 1):
            pdf_name = os.path.splitext(result['pdf_file'])[0]
            print(f"\nüìÑ [{i}/{len(self.base64_results)}] X·ª≠ l√Ω: {result['pdf_file']}")
            
            try:
                # T·∫°o t√™n file CSV
                csv_filename = f"sanctions_from_{pdf_name}_{self.timestamp}.csv"
                
                print(f"   üîÑ ƒêang g·ªçi Gemini API...")
                start_time = time.time()
                
                gemini_result = converter.convert_markdown_to_csv(
                    markdown_content=result['base64_content'],
                    output_path=csv_filename,
                    is_base64=True
                )
                
                elapsed_time = time.time() - start_time
                
                if not gemini_result['success']:
                    print(f"   ‚ùå Gemini API l·ªói: {gemini_result['error']}")
                    continue
                
                self.temp_files.append(csv_filename)
                successful_conversions += 1
                
                # Ph√¢n t√≠ch CSV results
                if os.path.exists(csv_filename):
                    file_size = os.path.getsize(csv_filename)
                    df = pd.read_csv(csv_filename)
                    
                    print(f"   ‚úÖ Th√†nh c√¥ng ({elapsed_time:.1f}s)")
                    print(f"   üìä CSV: {file_size:,} bytes, {len(df):,} records")
                    
                    # C·∫≠p nh·∫≠t th√¥ng tin processed
                    result['csv_file'] = csv_filename
                    result['csv_records'] = len(df)
                    result['processing_time'] = elapsed_time
                    
                    if 'Type' in df.columns:
                        type_counts = df['Type'].value_counts()
                        print(f"   üìã Types: {dict(type_counts)}")
                
            except Exception as e:
                print(f"   ‚ùå L·ªói: {e}")
        
        # T·ªïng k·∫øt b∆∞·ªõc 3
        print(f"\nüìä K·∫æT QU·∫¢ X·ª¨ L√ù GEMINI API:")
        print(f"   ‚úÖ Th√†nh c√¥ng: {successful_conversions}/{len(self.base64_results)}")
        
        total_records = sum(r.get('csv_records', 0) for r in self.base64_results if 'csv_records' in r)
        total_time = sum(r.get('processing_time', 0) for r in self.base64_results if 'processing_time' in r)
        
        print(f"   üìà T·ªïng records t·ª´ PDF: {total_records:,}")
        print(f"   ‚è±Ô∏è T·ªïng th·ªùi gian Gemini: {total_time:.1f}s ({total_time/60:.1f} ph√∫t)")
        
        if successful_conversions > 0:
            avg_time = total_time / successful_conversions
            print(f"   üìä Trung b√¨nh m·ªói file: {avg_time:.1f}s")
        
        return successful_conversions > 0
    
    def step4_ofac_extraction(self):
        """B∆∞·ªõc 4: Tr√≠ch xu·∫•t d·ªØ li·ªáu OFAC"""
        self.print_step(4, "TR√çCH XU·∫§T D·ªÆ LI·ªÜU OFAC")
        
        try:
            print("üá∫üá∏ ƒêang t·∫£i d·ªØ li·ªáu t·ª´ OFAC...")
            extractor = ofac_extractor.OFACSanctionsExtractor()
            df = extractor.run_extraction()
            
            if len(df) > 0:
                # T√¨m file OFAC m·ªõi nh·∫•t
                ofac_files = glob.glob("sanctions_cleaned_*.csv")
                if ofac_files:
                    latest_ofac = max(ofac_files, key=os.path.getctime)
                    # Ki·ªÉm tra xem file c√≥ ph·∫£i t·ª´ OFAC kh√¥ng
                    df_check = pd.read_csv(latest_ofac)
                    if len(df_check) > 0 and 'Watchlist' in df_check.columns:
                        first_watchlist = str(df_check['Watchlist'].iloc[0]).lower()
                        if 'ofac' in first_watchlist:
                            self.temp_files.append(latest_ofac)
                            print(f"‚úÖ OFAC extraction ho√†n t·∫•t: {latest_ofac}")
                            print(f"üìä OFAC records: {len(df_check):,}")
                return True
            else:
                print("‚ö†Ô∏è Kh√¥ng c√≥ d·ªØ li·ªáu m·ªõi t·ª´ OFAC")
                return True
                
        except Exception as e:
            print(f"‚ùå L·ªói tr√≠ch xu·∫•t OFAC: {e}")
            print("‚è≠Ô∏è Ti·∫øp t·ª•c v·ªõi b∆∞·ªõc ti·∫øp theo...")
            return True
    
    def step5_un_extraction(self):
        """B∆∞·ªõc 5: Tr√≠ch xu·∫•t d·ªØ li·ªáu UN"""
        self.print_step(5, "TR√çCH XU·∫§T D·ªÆ LI·ªÜU UN SANCTIONS")
        
        try:
            print("üåç ƒêang t·∫£i d·ªØ li·ªáu t·ª´ UN Security Council...")
            un_sanctions_parser.parse_un_sanctions_xml()
            
            # T√¨m file UN m·ªõi nh·∫•t
            un_files = glob.glob("sanctions_cleaned_*.csv")
            for file in un_files:
                if file not in self.temp_files:  # File m·ªõi
                    try:
                        df_check = pd.read_csv(file)
                        if len(df_check) > 0:
                            # Ki·ªÉm tra c√≥ ph·∫£i UN data kh√¥ng
                            if 'Source' in df_check.columns:
                                first_source = str(df_check['Source'].iloc[0]).lower()
                                if 'un' in first_source or 'security council' in first_source:
                                    self.temp_files.append(file)
                                    print(f"‚úÖ UN extraction ho√†n t·∫•t: {file}")
                                    print(f"üìä UN records: {len(df_check):,}")
                                    break
                    except Exception as e:
                        print(f"‚ö†Ô∏è Kh√¥ng th·ªÉ ki·ªÉm tra file {file}: {e}")
            
            return True
            
        except Exception as e:
            print(f"‚ùå L·ªói tr√≠ch xu·∫•t UN: {e}")
            print("‚è≠Ô∏è Ti·∫øp t·ª•c v·ªõi b∆∞·ªõc cu·ªëi...")
            return True
    
    def update_watchlist_column(self, df, filename):
        """C·∫≠p nh·∫≠t c·ªôt Watchlist d·ª±a tr√™n ngu·ªìn d·ªØ li·ªáu"""
        if 'Watchlist' not in df.columns:
            df['Watchlist'] = ''
        
        # X√°c ƒë·ªãnh ngu·ªìn d·ªØ li·ªáu
        watchlist_value = ''
        
        # Ki·ªÉm tra c√°c c·ªôt ƒë·∫∑c tr∆∞ng ƒë·ªÉ x√°c ƒë·ªãnh ngu·ªìn
        if 'Source' in df.columns:
            # UN data c√≥ c·ªôt Source
            first_source = str(df['Source'].iloc[0]).lower() if len(df) > 0 else ''
            if 'un' in first_source or 'security council' in first_source:
                watchlist_value = 'UN'
            
        if not watchlist_value and 'Watchlist' in df.columns:
            # OFAC data ho·∫∑c data ƒë√£ c√≥ watchlist
            first_watchlist = str(df['Watchlist'].iloc[0]).lower() if len(df) > 0 else ''
            if 'ofac' in first_watchlist or 'specially designated' in first_watchlist:
                watchlist_value = 'OFAC'
            elif first_watchlist and first_watchlist != 'nan':
                watchlist_value = df['Watchlist'].iloc[0]  # Keep existing value
        
        # EU data - t·ª´ file ƒë∆∞·ª£c x·ª≠ l√Ω b·ªüi Gemini (c√≥ pattern sanctions_from_)
        if not watchlist_value and 'sanctions_from_' in filename:
            # Extract EU code t·ª´ filename
            # Pattern: sanctions_from_202501578_timestamp.csv
            match = re.search(r'sanctions_from_(\d+)_', filename)
            if match:
                pdf_number = match.group(1)
                watchlist_value = self.extract_eu_watchlist_from_number(pdf_number)
            else:
                watchlist_value = 'EU'
        
        # Fallback
        if not watchlist_value:
            watchlist_value = 'Unknown'
        
        # C·∫≠p nh·∫≠t c·ªôt Watchlist
        df['Watchlist'] = watchlist_value
        
        print(f"   üè∑Ô∏è C·∫≠p nh·∫≠t Watchlist: {watchlist_value} cho {len(df)} records")
        return df
    
    def extract_eu_watchlist_from_number(self, pdf_number):
        """Tr√≠ch xu·∫•t m√£ EU t·ª´ s·ªë PDF (VD: 202501578 -> 2025/1578)"""
        if not pdf_number or len(pdf_number) < 8:
            return 'EU'
        
        # Pattern cho file EU: YYYYnnnnn (lo·∫°i b·ªè s·ªë 0 ƒë·ª©ng ƒë·∫ßu sau YYYY)
        if pdf_number.isdigit() and len(pdf_number) >= 8:
            year = pdf_number[:4]
            number = pdf_number[4:].lstrip('0')
            if number:  # Make sure number is not empty
                return f"{year}/{number}"
        
        return 'EU'
    
    def standardize_watchlist(self, df):
        """Chu·∫©n h√≥a v√† l√†m s·∫°ch c·ªôt Watchlist"""
        if 'Watchlist' not in df.columns:
            return df
        
        print("   üè∑Ô∏è ƒêang chu·∫©n h√≥a c√°c gi√° tr·ªã Watchlist...")
        
        # Th·ªëng k√™ tr∆∞·ªõc khi chu·∫©n h√≥a
        original_values = df['Watchlist'].value_counts()
        print("   üìä Gi√° tr·ªã Watchlist tr∆∞·ªõc chu·∫©n h√≥a:")
        for value, count in original_values.head(10).items():  # Show top 10 only
            print(f"      {value}: {count}")
        if len(original_values) > 10:
            print(f"      ... v√† {len(original_values) - 10} gi√° tr·ªã kh√°c")
        
        def clean_watchlist_value(value):
            if pd.isna(value) or not value:
                return 'Unknown'
            
            value_str = str(value).strip()
            
            # UN patterns
            if any(keyword in value_str.lower() for keyword in ['un', 'united nations', 'security council']):
                return 'UN'
            
            # OFAC patterns  
            if any(keyword in value_str.lower() for keyword in ['ofac', 'specially designated', 'treasury']):
                return 'OFAC'
            
            # EU patterns - ki·ªÉm tra pattern YYYY/NNNN
            eu_pattern = r'\d{4}/\d+'
            if re.search(eu_pattern, value_str):
                return value_str
            
            # EU general
            if 'eu' in value_str.lower():
                return 'EU'
            
            # Gi·ªØ nguy√™n n·∫øu kh√¥ng match pattern n√†o
            return value_str
        
        df['Watchlist'] = df['Watchlist'].apply(clean_watchlist_value)
        
        # Th·ªëng k√™ sau khi chu·∫©n h√≥a
        final_values = df['Watchlist'].value_counts()
        print("   üìä Gi√° tr·ªã Watchlist sau chu·∫©n h√≥a:")
        for value, count in final_values.head(10).items():
            print(f"      {value}: {count}")
        if len(final_values) > 10:
            print(f"      ... v√† {len(final_values) - 10} gi√° tr·ªã kh√°c")
        
        return df
    
    def step6_consolidate_data(self):
        """B∆∞·ªõc 6: T·ªïng h·ª£p t·∫•t c·∫£ d·ªØ li·ªáu"""
        self.print_step(6, "T·ªîNG H·ª¢P D·ªÆ LI·ªÜU CU·ªêI C√ôNG")
        
        try:
            # T√¨m t·∫•t c·∫£ file CSV
            all_csv_files = glob.glob("sanctions_cleaned_*.csv") + glob.glob("sanctions_from_*.csv")
            
            if not all_csv_files:
                print("‚ùå Kh√¥ng t√¨m th·∫•y file CSV n√†o ƒë·ªÉ t·ªïng h·ª£p")
                return False
            
            print(f"üìã T√¨m th·∫•y {len(all_csv_files)} file CSV:")
            
            # Group files by source for better reporting
            pdf_files = [f for f in all_csv_files if 'sanctions_from_' in f]
            ofac_files = [f for f in all_csv_files if 'sanctions_from_' not in f and 'ofac' in f.lower()]
            un_files = [f for f in all_csv_files if 'sanctions_from_' not in f and 'un' in f.lower()]
            other_files = [f for f in all_csv_files if f not in pdf_files + ofac_files + un_files]
            
            print(f"   üìÑ T·ª´ PDF: {len(pdf_files)} files")
            print(f"   üá∫üá∏ OFAC: {len(ofac_files)} files")
            print(f"   üåç UN: {len(un_files)} files")
            print(f"   üìã Kh√°c: {len(other_files)} files")
            
            # Hi·ªÉn th·ªã chi ti·∫øt files
            for file in all_csv_files:
                file_size = os.path.getsize(file)
                mod_time = datetime.fromtimestamp(os.path.getmtime(file))
                print(f"   üìÑ {file} ({file_size:,} bytes, {mod_time.strftime('%H:%M:%S')})")
            
            # ƒê·ªçc v√† t·ªïng h·ª£p t·∫•t c·∫£ file
            all_dataframes = []
            total_records = 0
            processing_summary = {
                'pdf_records': 0,
                'ofac_records': 0,
                'un_records': 0,
                'other_records': 0
            }
            
            for file in all_csv_files:
                try:
                    df = pd.read_csv(file, encoding='utf-8')
                    if len(df) > 0:
                        # X√°c ƒë·ªãnh ngu·ªìn d·ªØ li·ªáu v√† c·∫≠p nh·∫≠t Watchlist
                        df = self.update_watchlist_column(df, file)
                        all_dataframes.append(df)
                        total_records += len(df)
                        
                        # Ph√¢n lo·∫°i records
                        if 'sanctions_from_' in file:
                            processing_summary['pdf_records'] += len(df)
                        elif 'ofac' in file.lower():
                            processing_summary['ofac_records'] += len(df)
                        elif 'un' in file.lower():
                            processing_summary['un_records'] += len(df)
                        else:
                            processing_summary['other_records'] += len(df)
                        
                        print(f"   ‚úÖ {file}: {len(df)} records")
                    else:
                        print(f"   ‚ö†Ô∏è {file}: File tr·ªëng")
                except Exception as e:
                    print(f"   ‚ùå {file}: L·ªói ƒë·ªçc - {e}")
            
            if not all_dataframes:
                print("‚ùå Kh√¥ng c√≥ d·ªØ li·ªáu h·ª£p l·ªá ƒë·ªÉ t·ªïng h·ª£p")
                return False
            
            # T·ªïng h·ª£p DataFrame
            print(f"\nüîÑ ƒêang t·ªïng h·ª£p {total_records:,} records t·ª´ {len(all_dataframes)} file...")
            final_df = pd.concat(all_dataframes, ignore_index=True)
            
            # X·ª≠ l√Ω format c·ªôt DOB_DJ
            print("üîß ƒêang chu·∫©n h√≥a format ng√†y th√°ng...")
            final_df = self.standardize_dob_format(final_df)
            
            # Chu·∫©n h√≥a c·ªôt Watchlist
            print("üè∑Ô∏è ƒêang chu·∫©n h√≥a c·ªôt Watchlist...")
            final_df = self.standardize_watchlist(final_df)
            
            # Lo·∫°i b·ªè duplicate
            original_count = len(final_df)
            final_df = final_df.drop_duplicates(subset=['Name'], keep='first')
            duplicate_count = original_count - len(final_df)
            
            if duplicate_count > 0:
                print(f"üîÑ ƒê√£ lo·∫°i b·ªè {duplicate_count:,} b·∫£n ghi tr√πng l·∫∑p")
            
            # S·∫Øp x·∫øp theo t√™n
            final_df = final_df.sort_values('Name').reset_index(drop=True)
            
            # L∆∞u file cu·ªëi c√πng
            final_df.to_csv(self.final_output, index=False, encoding='utf-8-sig')
            
            print(f"\n‚úÖ T·ªîNG H·ª¢P HO√ÄN T·∫§T!")
            print(f"üìÅ File cu·ªëi c√πng: {self.final_output}")
            print(f"üìä T·ªïng s·ªë records: {len(final_df):,}")
            
            # Detailed breakdown
            print(f"\nüìà PH√ÇN T√çCH CHI TI·∫æT:")
            print(f"   üìÑ T·ª´ PDF files: {processing_summary['pdf_records']:,} records")
            print(f"   üá∫üá∏ OFAC data: {processing_summary['ofac_records']:,} records")
            print(f"   üåç UN data: {processing_summary['un_records']:,} records")
            print(f"   üìã Kh√°c: {processing_summary['other_records']:,} records")
            
            print(f"\nüìà Ph√¢n b·ªë theo Watchlist:")
            if 'Watchlist' in final_df.columns:
                watchlist_counts = final_df['Watchlist'].value_counts()
                for watchlist, count in watchlist_counts.items():
                    percentage = (count / len(final_df)) * 100
                    print(f"   {watchlist}: {count:,} ({percentage:.1f}%)")
            
            print(f"\nüìà Ph√¢n b·ªë theo Type:")
            if 'Type' in final_df.columns:
                type_counts = final_df['Type'].value_counts()
                for entity_type, count in type_counts.items():
                    percentage = (count / len(final_df)) * 100
                    print(f"   {entity_type}: {count:,} ({percentage:.1f}%)")
            
            # L∆∞u th√¥ng tin processing ƒë·ªÉ summary cu·ªëi
            self.final_summary = {
                'total_records': len(final_df),
                'duplicate_removed': duplicate_count,
                'processing_breakdown': processing_summary,
                'watchlist_distribution': dict(final_df['Watchlist'].value_counts()) if 'Watchlist' in final_df.columns else {},
                'type_distribution': dict(final_df['Type'].value_counts()) if 'Type' in final_df.columns else {}
            }
            
            return True
            
        except Exception as e:
            print(f"‚ùå L·ªói t·ªïng h·ª£p d·ªØ li·ªáu: {e}")
            return False
    
    def standardize_dob_format(self, df):
        """Chu·∫©n h√≥a format c·ªôt DOB_DJ th√†nh dd MMM yyyy"""
        if 'DOB_DJ' not in df.columns:
            return df
        
        def convert_date_format(date_str):
            if pd.isna(date_str) or not date_str or str(date_str).strip() == '':
                return ''
            
            date_str = str(date_str).strip()
            
            # N·∫øu ƒë√£ ƒë√∫ng format dd MMM yyyy, gi·ªØ nguy√™n
            if re.match(r'\d{1,2}\s+[A-Za-z]{3}\s+\d{4}', date_str):
                return date_str
            
            # C√°c format c·∫ßn convert
            date_formats = [
                '%d/%m/%Y',    # 12/10/1958
                '%d-%m-%Y',    # 12-10-1958
                '%d.%m.%Y',    # 12.10.1958
                '%Y-%m-%d',    # 1958-10-12
                '%Y/%m/%d',    # 1958/10/12
                '%m/%d/%Y',    # 10/12/1958
                '%d %m %Y',    # 12 10 1958
            ]
            
            for fmt in date_formats:
                try:
                    date_obj = datetime.strptime(date_str, fmt)
                    return date_obj.strftime('%d %b %Y')  # 12 Oct 1958
                except ValueError:
                    continue
            
            # N·∫øu kh√¥ng convert ƒë∆∞·ª£c, gi·ªØ nguy√™n
            return date_str
        
        print("   üîÑ ƒêang chu·∫©n h√≥a format DOB_DJ...")
        df['DOB_DJ'] = df['DOB_DJ'].apply(convert_date_format)
        converted_count = df['DOB_DJ'].apply(lambda x: bool(re.match(r'\d{1,2}\s+[A-Za-z]{3}\s+\d{4}', str(x)))).sum()
        print(f"   üìÖ ƒê√£ chu·∫©n h√≥a {converted_count:,} ng√†y th√°ng")
        
        return df
    
    def cleanup_temp_files(self):
        """D·ªçn d·∫πp file t·∫°m"""
        print("\nüßπ D·ªåNG D·∫∏P FILE T·∫†M")
        print("-" * 30)
        
        temp_patterns = [
            "sanctions_cleaned_*_*.csv",  # File c√≥ timestamp chi ti·∫øt
            "sanctions_from_*.csv",       # File t·ª´ Gemini
            "*.md"                        # File markdown
        ]
        
        cleanup_files = []
        for pattern in temp_patterns:
            cleanup_files.extend(glob.glob(pattern))
        
        # Kh√¥ng x√≥a file cu·ªëi c√πng
        cleanup_files = [f for f in cleanup_files if f != self.final_output]
        
        if cleanup_files:
            print(f"üóëÔ∏è T√¨m th·∫•y {len(cleanup_files)} file t·∫°m:")
            
            # Group files by type for better display
            md_files = [f for f in cleanup_files if f.endswith('.md')]
            csv_files = [f for f in cleanup_files if f.endswith('.csv')]
            
            if md_files:
                print(f"   üìù Markdown files: {len(md_files)}")
                for file in md_files[:3]:  # Show first 3
                    print(f"      üìÑ {file}")
                if len(md_files) > 3:
                    print(f"      ... v√† {len(md_files) - 3} file MD kh√°c")
            
            if csv_files:
                print(f"   üìä CSV files: {len(csv_files)}")
                for file in csv_files[:3]:  # Show first 3
                    print(f"      üìÑ {file}")
                if len(csv_files) > 3:
                    print(f"      ... v√† {len(csv_files) - 3} file CSV kh√°c")
            
            try:
                choice = input(f"\nX√≥a t·∫•t c·∫£ {len(cleanup_files)} file t·∫°m? (y/N): ").strip().lower()
                if choice in ['y', 'yes']:
                    deleted_count = 0
                    for file in cleanup_files:
                        try:
                            os.remove(file)
                            deleted_count += 1
                        except Exception as e:
                            print(f"   ‚ùå L·ªói x√≥a {file}: {e}")
                    
                    print(f"   ‚úÖ ƒê√£ x√≥a {deleted_count}/{len(cleanup_files)} file")
                else:
                    print("   ‚è≠Ô∏è Gi·ªØ l·∫°i c√°c file t·∫°m")
            except KeyboardInterrupt:
                print("\n   ‚è≠Ô∏è B·ªè qua d·ªçn d·∫πp")
        else:
            print("‚ú® Kh√¥ng c√≥ file t·∫°m c·∫ßn d·ªçn d·∫πp")
    
    def run_workflow(self):
        """Ch·∫°y to√†n b·ªô workflow v·ªõi h·ªó tr·ª£ batch processing"""
        self.print_header("SANCTIONS PROCESSING WORKFLOW - MULTI PDF SUPPORT")
        
        print("üéØ Workflow h·ªó tr·ª£ x·ª≠ l√Ω nhi·ªÅu PDF:")
        print("   1. Chuy·ªÉn PDF sang Markdown")
        print("   2. Chuy·ªÉn Markdown sang Base64")  
        print("   3. X·ª≠ l√Ω Base64 v·ªõi Gemini AI ‚Üí CSV")
        print("   4. T·∫£i d·ªØ li·ªáu OFAC")
        print("   5. T·∫£i d·ªØ li·ªáu UN Sanctions")
        print("   6. T·ªïng h·ª£p t·∫•t c·∫£ th√†nh 1 file CSV")
        
        # Ch·ªçn ch·∫ø ƒë·ªô v√† l·∫•y input files
        if not self.get_processing_mode():
            return False
        
        # L·∫•y Gemini API key
        if not self.get_gemini_api_key():
            print("‚ö†Ô∏è Ti·∫øp t·ª•c kh√¥ng c√≥ Gemini API - s·∫Ω b·ªè qua x·ª≠ l√Ω PDF")
        
        # Hi·ªÉn th·ªã t√≥m t·∫Øt
        print(f"\nüìã T√ìM T·∫ÆT X·ª¨ L√ù:")
        print(f"   üìÅ Ch·∫ø ƒë·ªô: {self.processing_mode}")
        print(f"   üìÑ S·ªë PDF: {len(self.pdf_files)}")
        print(f"   ü§ñ Gemini API: {'‚úÖ C√≥' if self.gemini_api_key else '‚ùå Kh√¥ng'}")
        
        if self.processing_mode == 'batch':
            total_size = sum(os.path.getsize(f) for f in self.pdf_files)
            print(f"   üìä T·ªïng k√≠ch th∆∞·ªõc PDF: {total_size:,} bytes ({total_size/1024/1024:.1f} MB)")
            
            # Estimate processing time
            estimated_time = len(self.pdf_files) * 2  # 2 minutes per PDF estimate
            print(f"   ‚è±Ô∏è Th·ªùi gian ∆∞·ªõc t√≠nh: ~{estimated_time} ph√∫t")
        
        # X√°c nh·∫≠n ti·∫øp t·ª•c
        try:
            if self.processing_mode == 'single':
                confirm_msg = f"üöÄ B·∫Øt ƒë·∫ßu x·ª≠ l√Ω {self.pdf_files[0]}? (Y/n): "
            else:
                confirm_msg = f"üöÄ B·∫Øt ƒë·∫ßu x·ª≠ l√Ω {len(self.pdf_files)} PDF files? (Y/n): "
                
            confirm = input(confirm_msg).strip().lower()
            if confirm and confirm not in ['y', 'yes']:
                print("‚ùå ƒê√£ h·ªßy workflow")
                return False
        except KeyboardInterrupt:
            print("\n‚ùå ƒê√£ h·ªßy workflow")
            return False
        
        start_time = time.time()
        success_steps = 0
        total_steps = 6
        
        # Ch·∫°y t·ª´ng b∆∞·ªõc v·ªõi batch support
        if self.processing_mode == 'batch':
            steps = [
                self.step1_pdf_to_md_batch,
                self.step2_md_to_base64_batch,
                self.step3_base64_to_csv_via_gemini_batch,
                self.step4_ofac_extraction,
                self.step5_un_extraction,
                self.step6_consolidate_data
            ]
        else:
            # Single mode - use original methods adapted
            steps = [
                self.step1_pdf_to_md_single,
                self.step2_md_to_base64_single,
                self.step3_base64_to_csv_via_gemini_single,
                self.step4_ofac_extraction,
                self.step5_un_extraction,
                self.step6_consolidate_data
            ]
        
        for i, step_func in enumerate(steps, 1):
            try:
                # B·ªè qua b∆∞·ªõc 3 n·∫øu kh√¥ng c√≥ API key
                if i == 3 and not self.gemini_api_key:
                    print(f"‚è≠Ô∏è B·ªè qua b∆∞·ªõc {i} - kh√¥ng c√≥ Gemini API key")
                    continue
                
                if step_func():
                    success_steps += 1
                    print(f"‚úÖ B∆∞·ªõc {i} ho√†n th√†nh")
                else:
                    print(f"‚ùå B∆∞·ªõc {i} th·∫•t b·∫°i")
                    if i in [1, 2, 6]:  # B∆∞·ªõc quan tr·ªçng
                        print("üõë D·ª´ng workflow do l·ªói quan tr·ªçng")
                        return False
                    elif i == 3:  # Gemini step kh√¥ng b·∫Øt bu·ªôc
                        print("‚è≠Ô∏è Ti·∫øp t·ª•c v·ªõi c√°c b∆∞·ªõc kh√°c...")
            except KeyboardInterrupt:
                print(f"\nüõë Ng∆∞·ªùi d√πng d·ª´ng workflow t·∫°i b∆∞·ªõc {i}")
                return False
            except Exception as e:
                print(f"‚ùå L·ªói kh√¥ng mong mu·ªën t·∫°i b∆∞·ªõc {i}: {e}")
                if i in [1, 2, 6]:  # B∆∞·ªõc quan tr·ªçng
                    return False
                else:
                    print("‚è≠Ô∏è Ti·∫øp t·ª•c v·ªõi b∆∞·ªõc ti·∫øp theo...")
        
        # T·ªïng k·∫øt
        elapsed_time = time.time() - start_time
        self.print_header("K·∫æT QU·∫¢ WORKFLOW")
        
        print(f"‚è±Ô∏è Th·ªùi gian th·ª±c hi·ªán: {elapsed_time:.1f} gi√¢y ({elapsed_time/60:.1f} ph√∫t)")
        print(f"‚úÖ Ho√†n th√†nh: {success_steps}/{total_steps} b∆∞·ªõc")
        
        if success_steps >= 4:  # T·ªëi thi·ªÉu c·∫ßn 4 b∆∞·ªõc ƒë·ªÉ c√≥ k·∫øt qu·∫£
            print("üéâ WORKFLOW HO√ÄN TH√ÄNH TH√ÄNH C√îNG!")
            
            if os.path.exists(self.final_output):
                file_size = os.path.getsize(self.final_output)
                df = pd.read_csv(self.final_output)
                print(f"üìÅ File cu·ªëi c√πng: {self.final_output}")
                print(f"üìä K√≠ch th∆∞·ªõc: {file_size:,} bytes")
                print(f"üìà S·ªë records: {len(df):,}")
                
                # Hi·ªÉn th·ªã summary chi ti·∫øt cho batch mode
                if self.processing_mode == 'batch' and hasattr(self, 'final_summary'):
                    print(f"\nüéØ CHI TI·∫æT X·ª¨ L√ù BATCH:")
                    print(f"   üìÑ PDF files x·ª≠ l√Ω: {len(self.pdf_files)}")
                    
                    if hasattr(self, 'base64_results'):
                        successful_pdf = len([r for r in self.base64_results if 'csv_file' in r])
                        print(f"   ‚úÖ PDF th√†nh c√¥ng: {successful_pdf}/{len(self.pdf_files)}")
                        
                        total_pdf_records = self.final_summary['processing_breakdown']['pdf_records']
                        print(f"   üìä Records t·ª´ PDF: {total_pdf_records:,}")
                        
                        if successful_pdf > 0:
                            avg_records = total_pdf_records / successful_pdf
                            print(f"   üìà Trung b√¨nh m·ªói PDF: {avg_records:.0f} records")
                
                # Hi·ªÉn th·ªã breakdown cu·ªëi
                print(f"\nüìã SUMMARY CHI TI·∫æT:")
                print(f"   üéØ PDF x·ª≠ l√Ω: {len(self.pdf_files)} files")
                print(f"   üìÑ Markdown: {'‚úÖ T·∫°o th√†nh c√¥ng' if hasattr(self, 'conversion_results') else '‚ùå Th·∫•t b·∫°i'}")
                print(f"   üîí Base64: {'‚úÖ Chuy·ªÉn ƒë·ªïi th√†nh c√¥ng' if hasattr(self, 'base64_results') else '‚ùå Th·∫•t b·∫°i'}")
                print(f"   ü§ñ Gemini API: {'‚úÖ X·ª≠ l√Ω th√†nh c√¥ng' if self.gemini_api_key else '‚è≠Ô∏è B·ªè qua'}")
                print(f"   üá∫üá∏ OFAC: ‚úÖ T√≠ch h·ª£p")
                print(f"   üåç UN: ‚úÖ T√≠ch h·ª£p")
                
                # Performance metrics
                if self.processing_mode == 'batch' and elapsed_time > 0:
                    files_per_minute = (len(self.pdf_files) * 60) / elapsed_time
                    print(f"   üìä Hi·ªáu su·∫•t: {files_per_minute:.1f} PDF/ph√∫t")
                
                # D·ªçn d·∫πp file t·∫°m
                self.cleanup_temp_files()
                
                return True
        else:
            print("‚ö†Ô∏è Workflow ho√†n th√†nh m·ªôt ph·∫ßn")
            print("üí° Ki·ªÉm tra l·∫°i c√°c b∆∞·ªõc b·ªã l·ªói")
            print(f"üí° C·∫ßn √≠t nh·∫•t 4/{total_steps} b∆∞·ªõc ƒë·ªÉ c√≥ k·∫øt qu·∫£ h·ª£p l·ªá")
            return False
    
    # Single mode methods (adapted from batch methods)
    def step1_pdf_to_md_single(self):
        """Single PDF conversion - wrapper cho batch method"""
        return self.step1_pdf_to_md_batch()
    
    def step2_md_to_base64_single(self):
        """Single MD to Base64 - wrapper cho batch method"""
        return self.step2_md_to_base64_batch()
    
    def step3_base64_to_csv_via_gemini_single(self):
        """Single Base64 to CSV - wrapper cho batch method"""
        return self.step3_base64_to_csv_via_gemini_batch()

def main():
    """H√†m main v·ªõi multi-PDF support"""
    try:
        workflow = SanctionsWorkflow()
        success = workflow.run_workflow()
        
        if success:
            print("\nüéä C·∫¢M ∆†N B·∫†N ƒê√É S·ª¨ D·ª§NG SANCTIONS PROCESSING SYSTEM!")
            print("üí° Workflow ƒë√£ ƒë∆∞·ª£c n√¢ng c·∫•p v·ªõi h·ªó tr·ª£:")
            print("   ‚úÖ X·ª≠ l√Ω nhi·ªÅu PDF c√πng l√∫c")
            print("   ‚úÖ Batch processing t·ª± ƒë·ªông")
            print("   ‚úÖ PDF ‚Üí Markdown ‚Üí Base64 ‚Üí CSV (via Gemini API)")
            print("   ‚úÖ T√≠ch h·ª£p OFAC v√† UN data")
            sys.exit(0)
        else:
            print("\nüíî Workflow kh√¥ng ho√†n th√†nh. Vui l√≤ng ki·ªÉm tra l·∫°i.")
            sys.exit(1)
            
    except KeyboardInterrupt:
        print("\n\nüëã T·∫°m bi·ªát!")
        sys.exit(0)
    except Exception as e:
        print(f"\nüí• L·ªói h·ªá th·ªëng: {e}")
        sys.exit(1)

if __name__ == "__main__":
    main()